# ðŸŽ™ï¸ WiDS 5.0 â€” Speech to Text Engine

This repository presents my work completed as part of **WiDS (Winter in Data Science) 5.0**, under the project:

> **UID: 40 â€” Speech to Text Engine**

The repository documents **weekly progress, implementations, and learning outcomes** achieved throughout the program, covering the journey from **NLP fundamentals** to building a **complete Speech-to-Text (STT) system**.

---

## ðŸ“Œ Table of Contents
- [Program Overview](#program-overview)
- [Week 1 â€” Foundations of NLP](#-week-1--foundations-of-natural-language-processing)
- [Week 2 â€” Neural Networks for NLP](#-week-2--neural-networks-for-nlp)
- [Week 3 â€” Transformer-Based NLP Models](#-week-3--transformer-based-nlp-models)
- [Week 4 â€” Fundamentals of Speech Processing](#-week-4--fundamentals-of-speech-processing)
- [Week 5 â€” Developing a Speech-to-Text System](#-week-5--developing-a-speech-to-text-system)
- [Notes](#-notes)

---

## ðŸ“– Program Overview

**WiDS 5.0** is a structured data science program focused on building strong theoretical foundations and practical skills in **Natural Language Processing** and **Speech Technologies**.  
This project emphasizes **conceptual clarity**, **hands-on implementation**, and **model evaluation**.

---

## ðŸ“… Week 1 â€” Foundations of Natural Language Processing

### Topics Covered
- Core concepts of Natural Language Processing (NLP)
- Tokenization and stopword handling
- Stemming and lemmatization techniques
- N-gram based text representation
- Fundamental text preprocessing workflows

---

## ðŸ“… Week 2 â€” Neural Networks for NLP

### Topics Covered
- Word embedding techniques: **Word2Vec**, **GloVe**
- Bag-of-Words (BoW) and **TF-IDF**
- Sentiment classification using **LSTM models**
- Embedding visualization using **PCA** and **t-SNE**
- Comparative evaluation of traditional vs neural NLP models

---

## ðŸ“… Week 3 â€” Transformer-Based NLP Models

### Topics Covered
- Transformer architecture and **self-attention mechanism**
- Comparison of **BERT**, **GPT**, and **Encoderâ€“Decoder** models
- Subword tokenization methods:
  - WordPiece  
  - Byte Pair Encoding (BPE)  
  - SentencePiece
- Fine-tuning transformer models for classification
- Using **Hugging Face pipelines**
- Abstractive text summarization with **BART**

---

## ðŸ“… Week 4 â€” Fundamentals of Speech Processing

### Topics Covered
- Digital representation of speech signals
- Sampling, quantization, and framing
- Timeâ€“frequency analysis using **spectrograms**
- Mel scale fundamentals and **Mel spectrograms**
- **MFCC** feature extraction
- Traditional vs deep learning-based ASR systems
- Audio preprocessing pipelines
- Introduction to **keyword spotting**

---

## ðŸ“… Week 5 â€” Developing a Speech-to-Text System

### Topics Covered
- Overview of Speech-to-Text (STT) technology
- End-to-end STT processing pipelines
- Use of **pretrained speech recognition models**
- Architecture and working of:
  - **Whisper**
  - **Wav2Vec 2.0**
- Speech data preparation and preprocessing
- Fine-tuning pretrained STT models
- Model evaluation using **Word Error Rate (WER)**

---

## âœ… Notes
- The project follows the structured weekly plan of **WiDS 5.0**
- Implementations prioritize **learning outcomes and interpretability**
- Lightweight datasets and models are used for **efficient execution**
- Ideal for beginners transitioning into **Speech & NLP research**

---

### ðŸ“¬ Author
**WiDS 5.0 Participant**  
_Project UID: 40 â€” Speech to Text Engine_







