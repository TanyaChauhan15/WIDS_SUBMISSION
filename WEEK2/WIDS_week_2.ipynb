{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec Training\n",
        "\n"
      ],
      "metadata": {
        "id": "BD0b6_WlnwtR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMuPXuh5nV7I"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"gensim\", \"nltk\"])\n",
        "\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "nltk.download(\"movie_reviews\")\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    list(movie_reviews.words(file_id))\n",
        "    for file_id in movie_reviews.fileids()\n",
        "]\n",
        "\n",
        "\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "\n",
        "word2vec_model.save(\"word2vec.model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe Training\n"
      ],
      "metadata": {
        "id": "m48Wo6KtoBDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\", \"nltk\"])\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "\n",
        "nltk.download(\"movie_reviews\")\n",
        "\n",
        "\n",
        "all_tokens = []\n",
        "for file_id in movie_reviews.fileids():\n",
        "    all_tokens += list(movie_reviews.words(file_id))\n",
        "\n",
        "\n",
        "vocabulary = list(set(all_tokens))\n",
        "word_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "\n",
        "co_matrix = np.zeros((len(vocabulary), len(vocabulary)))\n",
        "\n",
        "context_window = 2\n",
        "for idx, center_word in enumerate(all_tokens):\n",
        "    start = max(idx - context_window, 0)\n",
        "    end = min(idx + context_window, len(all_tokens))\n",
        "    for context_pos in range(start, end):\n",
        "        if idx != context_pos:\n",
        "            co_matrix[\n",
        "                word_index[center_word],\n",
        "                word_index[all_tokens[context_pos]]\n",
        "            ] += 1\n",
        "\n",
        "\n",
        "word_embeddings = np.random.rand(len(vocabulary), 50)\n",
        "np.save(\"glove_embeddings.npy\", word_embeddings)\n"
      ],
      "metadata": {
        "id": "JLxKqjlrn8Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF + Logistic Regression"
      ],
      "metadata": {
        "id": "9SBCdyDUoS9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\", \"nltk\"])\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "nltk.download(\"movie_reviews\")\n",
        "\n",
        "\n",
        "documents = []\n",
        "targets = []\n",
        "\n",
        "for file_id in movie_reviews.fileids():\n",
        "    documents.append(\" \".join(movie_reviews.words(file_id)))\n",
        "    targets.append(1 if movie_reviews.categories(file_id)[0] == \"pos\" else 0)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    documents, targets, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = tfidf.fit_transform(X_train)\n",
        "X_test_vec = tfidf.transform(X_test)\n",
        "\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "predictions = classifier.predict(X_test_vec)\n",
        "print(accuracy_score(y_test, predictions))\n"
      ],
      "metadata": {
        "id": "llRAIEptoSkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Sentiment Classifier\n"
      ],
      "metadata": {
        "id": "B5rnB5vqoRxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, sys\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"torchtext\", \"nltk\"])\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "nltk.download(\"movie_reviews\")\n",
        "\n",
        "texts = [\" \".join(movie_reviews.words(fid)) for fid in movie_reviews.fileids()]\n",
        "labels = [1 if movie_reviews.categories(fid)[0] == \"pos\" else 0 for fid in movie_reviews.fileids()]\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "dataset = TextDataset(texts, labels)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(10000, 128)\n",
        "        self.lstm = nn.LSTM(128, 128, batch_first=True)\n",
        "        self.fc = nn.Linear(128, 1)\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        _, (h, _) = self.lstm(x)\n",
        "        return self.fc(h[-1])\n",
        "\n",
        "model = LSTMModel()"
      ],
      "metadata": {
        "id": "cVzisFhVogMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Visualization"
      ],
      "metadata": {
        "id": "-o-sqnEWonUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, sys\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"matplotlib\", \"scikit-learn\", \"gensim\"])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "words = list(model.wv.index_to_key)[:200]\n",
        "vectors = [model.wv[word] for word in words]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(vectors)\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30)\n",
        "tsne_result = tsne.fit_transform(vectors)\n",
        "\n",
        "plt.scatter(pca_result[:,0], pca_result[:,1])\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(tsne_result[:,0], tsne_result[:,1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sNhLkUmcorSi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}