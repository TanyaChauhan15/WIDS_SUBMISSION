{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG4QvVdxpCxi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "def encode_text(batch):\n",
        "    return bert_tokenizer(\n",
        "        batch[\"text\"],\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "\n",
        "encoded_dataset = imdb_dataset.map(encode_text, batched=True)\n",
        "\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=bert_model,\n",
        "    args=train_args,\n",
        "    train_dataset=encoded_dataset[\"train\"]\n",
        "        .shuffle(seed=42)\n",
        "        .select(range(2000)),\n",
        "    eval_dataset=encoded_dataset[\"test\"]\n",
        "        .select(range(1000)),\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=\"data.txt\",\n",
        "    model_prefix=\"wids_tokenizer\",\n",
        "    vocab_size=2000,\n",
        "    model_type=\"bpe\"\n",
        ")\n",
        "\n",
        "print(\"SentencePiece tokenizer trained successfully.\")\n"
      ],
      "metadata": {
        "id": "sLhQ0yJ3puTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "result = classifier(\"Transformers are amazing!\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ToIXZ1slpyEJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}